## KeyBERT란?

KeyBERT는 문서에서 핵심 키워드를 추출하기 위해 만들어진 간단하면서도 강력한 키워드 추출 라이브러리입니다. KeyBERT는 사전 훈련된 Transformer 기반의 BERT 모델을 활용하여 문서에서 가장 중요한 키워드를 선정합니다. 특히 짧은 문서, 블로그 게시물, 논문 초록 등에서 효과적으로 작동합니다.

## KeyBERT의 원리

KeyBERT는 다음과 같은 과정을 거쳐 키워드를 추출합니다.

### 1. 문서 임베딩(Document Embedding)

전체 문서를 사전 학습된 BERT 모델을 통해 하나의 벡터로 표현합니다. 이 벡터는 문서 전체의 의미를 압축하여 나타냅니다.

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
doc_embedding = model.encode("KeyBERT is a minimal and easy-to-use keyword extraction technique.")
```

### 2. 후보 키워드 생성

문서에서 후보 키워드를 n-gram(연속된 단어 조합)으로 생성합니다. 예를 들어, 문장 "KeyBERT is useful"에서 가능한 1~2개의 단어 키워드는 'KeyBERT', 'is', 'useful', 'KeyBERT is', 'is useful' 등이 될 수 있습니다.

```python
from sklearn.feature_extraction.text import CountVectorizer

n_gram_range = (1, 2)
count = CountVectorizer(ngram_range=n_gram_range).fit(["KeyBERT is useful"])
candidates = count.get_feature_names_out()
print(candidates)
```

출력:
```
['is' 'is useful' 'keybert' 'keybert is' 'useful']
```

### 3. 키워드 임베딩(Keyword Embedding)

생성된 각 후보 키워드를 다시 BERT를 통해 벡터화합니다. 이를 통해 각 키워드의 의미가 벡터 형태로 표현됩니다.

```python
candidate_embeddings = model.encode(candidates)
```

### 4. 코사인 유사도 계산 및 키워드 선택

문서 임베딩과 후보 키워드 임베딩 간의 코사인 유사도를 계산하여, 유사도가 높은 키워드를 추출합니다.

```python
from sklearn.metrics.pairwise import cosine_similarity

cosine_scores = cosine_similarity([doc_embedding], candidate_embeddings)

# 높은 점수 순으로 정렬
sorted_keywords = [(candidates[i], cosine_scores[0][i]) for i in cosine_scores.argsort()[0][::-1]]
print(sorted_keywords[:5])
```

간단한 그림으로 나타내면 다음과 같습니다.

```
[문서] → [문서 임베딩]
              ↓
      [코사인 유사도 계산] ← [후보 키워드 임베딩]
              ↓
          [최종 키워드]
```

## 다양한 키워드 추출 기법

### Maximal Marginal Relevance (MMR)

MMR은 문서와의 관련성을 최대화하면서 선택된 키워드 간의 중복을 최소화하여 다양성을 확보하는 방법입니다.

수식은 다음과 같습니다.

\[ \text{MMR} = \lambda \cdot \text{Sim}(d_i, Q) - (1 - \lambda) \cdot \max_{d_j \in S}\text{Sim}(d_i, d_j) \]

- \( \text{Sim}(d_i, Q) \): 문서 \( Q \)와 후보 키워드 \( d_i \) 간의 유사도
- \( \max_{d_j \in S}\text{Sim}(d_i, d_j) \): 이미 선택된 키워드 \(S\)와 후보 키워드 \( d_i \) 사이의 최대 유사도
- \( \lambda \): 다양성과 관련성 간의 균형을 조정하는 파라미터 (0 ≤ \( \lambda \) ≤ 1)

### Max Sum Distance (MSD)

MSD는 후보 키워드 간 거리를 최대화하여 키워드 간 중복을 최소화합니다. 즉, 키워드 집합 내 모든 키워드 쌍 간의 유사도를 최소화하여 다양성을 극대화합니다.

수식은 다음과 같습니다.

\[ \text{Max Sum Distance} = \sum_{i=1}^{n}\sum_{j=i+1}^{n}(1 - \text{Sim}(d_i, d_j)) \]

- \( \text{Sim}(d_i, d_j) \): 후보 키워드 \( d_i \)와 \( d_j \) 간의 유사도

## KeyBERT와 LLM (Large Language Models)

KeyBERT는 최근 다양한 LLM을 활용하여 키워드 추출의 성능을 높일 수 있습니다. OpenAI의 GPT 모델, Hugging Face의 모델 등 최신 LLM을 통합하여 사용할 수 있습니다.

예제 코드:

```python
from keybert.llm import OpenAI
from keybert import KeyLLM

llm = OpenAI(model="text-davinci-003")
keyllm = KeyLLM(llm)

doc = "KeyBERT is a powerful keyword extraction tool leveraging large language models."
keywords = keyllm.extract_keywords(doc, top_n=5)
print(keywords)
```

## 결론

KeyBERT는 간편하면서도 효율적으로 문서에서 중요한 키워드를 추출할 수 있는 도구입니다. 특히 의미론적인 키워드를 추출하여 검색 엔진 최적화(SEO), 콘텐츠 분석, 문서 분류 등 다양한 분야에서 활용할 수 있습니다.

### reference
- https://velog.io/@about2weeks/keybert%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%95%9C%EA%B8%80-%EB%AC%B8%EC%9E%A5-%ED%82%A4%EC%9B%8C%EB%93%9C-%EC%B6%94%EC%B6%9C
- https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf