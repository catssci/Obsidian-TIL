# 📌 LLM 학습 로드맵 & 커리큘럼

**8개월~12개월** 정도의 기간을 가정한 실제 커리큘럼입니다. 주차별 과제와 권장 자료를 구체적으로 나열했으며, 주간 목표를 달성하며 단계적으로 학습할 수 있도록 구성했습니다.

---
## 전체 개요
| 파트                           | 주차 범위   | 주요 내용                      |
| ---------------------------- | ------- | -------------------------- |
| **Part 1: LLM Fundamentals** | 1~8주차   | 수학/통계 & Python/기본 ML 개념    |
| **Part 2: Neural Networks**  | 9~14주차  | 신경망, CNN, RNN, 옵티마이저, 규제화  |
| **Part 3: NLP**              | 15~20주차 | 전통적 NLP, 텍스트 전처리, 시퀀스 모델   |
| **Part 4: LLM Scientist**    | 21~28주차 | 트랜스포머, 대규모 언어 모델 연구, 분산 학습 |
| **Part 5: LLM Engineer**     | 29~36주차 | 애플리케이션 개발, 최적화 & 배포, MLOps |

> 본 커리큘럼은 최소 36주(약 8~9개월)를 기준으로 잡았습니다. 상황에 따라 연장(최대 12개월 이상) 가능하며, 개인 실력과 리소스에 따라 주차별로 압축/확장할 수 있습니다.

---
## Part 1: LLM Fundamentals (1~8주차)

### Week 1-2: Python 기초 & Numpy/Pandas
- **학습 목표**
  - Python 기본 문법, 자료형, 제어문, 함수 숙지
  - Numpy와 Pandas 활용하여 데이터 처리
- **학습 자료**
  - [Python 공식 문서](https://docs.python.org/3/)
  - [CS231n: Python Numpy Tutorial](http://cs231n.github.io/python-numpy-tutorial/)
  - [Kaggle Python Micro-Course](https://www.kaggle.com/learn/python)
- **과제/실습**
  1. Python 기본 문법 문제 풀이 (조건문, 반복문, 함수 만들기)
  2. Numpy로 행렬 연산 실습 (벡터, 행렬)
  3. Pandas로 간단한 EDA(Exploratory Data Analysis) 진행 (결측치 처리, 통계량 확인)

### Week 3-4: 선형대수 & 미적분 기초
- **학습 목표**
  - 선형대수(벡터, 행렬, 행렬식, 고유값 등)와 미적분(미분, 편미분, 체인룰) 이해
  - Python으로 간단한 수치 계산 구현
- **학습 자료**
  - [Mathematics for Machine Learning (Free PDF)](https://mml-book.github.io/)
  - [3Blue1Brown - Essence of Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra)
  - [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1)
- **과제/실습**
  1. 행렬 연산 함수를 직접 구현 (덧셈, 곱셈, 전치, 역행렬 등)
  2. 미분/편미분 예제 풀이 (단변수/다변수 함수)
  3. 경사하강법(Gradient Descent) 기본 알고리즘 구현

### Week 5-6: 확률통계 & 기본 ML
- **학습 목표**
  - 확률 분포, 기대값, 분산, 베이즈 정리 이해
  - 기초 머신러닝(회귀, 분류) 알고리즘 개념 습득
- **학습 자료**
  - [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability)
  - [scikit-learn 공식 문서](https://scikit-learn.org/stable/)
- **과제/실습**
  1. 정규분포, 이항분포, 카이제곱 분포 등에 대한 간단 예제
  2. 베이즈 정리를 코드로 구현해 특정 이벤트 확률 추론
  3. scikit-learn으로 회귀/분류(로지스틱 회귀, KNN 등) 모델 학습 및 평가

### Week 7-8: 프로젝트(ML 기초 종합)
- **학습 목표**
  - 지금까지 배운 Python, 수학/통계, 기본 ML 지식을 활용하여 간단 프로젝트
- **프로젝트 아이디어 예시**
  1. **타이타닉 생존자 예측**: scikit-learn 분류 모델 비교 & 하이퍼파라미터 튜닝
  2. **Housing Price 예측**: 회귀 모델 적용, RMSE 비교
  3. **이상치 탐지**: 군집화 기반 이상치 판별
- **출력물**
  - Jupyter Notebook 또는 Python 스크립트 + 간단한 PPT 정리

---
## Part 2: Neural Networks (9~14주차)

### Week 9-10: 퍼셉트론 & MLP
- **학습 목표**
  - 신경망의 기초(퍼셉트론, 역전파)
  - Dense Layer를 활용한 다층 퍼셉트론(MLP) 이해
- **학습 자료**
  - [Deep Learning (Ian Goodfellow 등)](https://www.deeplearningbook.org/)
  - [fast.ai - Intro to Neural Networks](https://course.fast.ai/)
- **과제/실습**
  1. 퍼셉트론 논리회로(AND, OR, XOR) 직접 구현
  2. 역전파 알고리즘을 코드로 구현 (수식 → 코드)
  3. MLP로 MNIST 분류 실습 (PyTorch/Keras 중 택1)

### Week 11-12: CNN & RNN
- **학습 목표**
  - 합성곱 신경망(CNN), 순환 신경망(RNN), LSTM/GRU 구조와 활용
- **학습 자료**
  - [cs231n - CNN 강의자료](http://cs231n.stanford.edu/)
  - [cs224n - RNN 강의자료](http://web.stanford.edu/class/cs224n/)
- **과제/실습**
  1. CNN으로 CIFAR-10 이미지 분류 실험
  2. RNN/LSTM으로 간단한 텍스트 생성(문장 완성) 실습

### Week 13-14: 옵티마이저 & 규제화
- **학습 목표**
  - Adam, RMSProp, Learning Rate Scheduler 등 다양 옵티마이저 사용
  - Dropout, Batch Normalization, Early Stopping 등 규제화 기법
- **학습 자료**
  - [PyTorch Optimizer 문서](https://pytorch.org/docs/stable/optim.html)
  - [Keras Callbacks & Regularization](https://keras.io/api/callbacks/)
- **과제/실습**
  1. MNIST/CIFAR-10 모델에 다양한 옵티마이저 적용, 결과 비교
  2. Dropout/BatchNorm 유무에 따른 성능 차이 비교
  3. TensorBoard로 학습 곡선 시각화

---
## Part 3: NLP (15~20주차)

### Week 15-16: 전통적 NLP & 텍스트 전처리
- **학습 목표**
  - 토큰화, 정규화, 불용어 제거, n-그램 모델
  - TF-IDF, Word2Vec, GloVe 등 임베딩 개념
- **학습 자료**
  - [NLTK 공식 문서](https://www.nltk.org/)
  - [Stanford CS224n](http://web.stanford.edu/class/cs224n/)
  - [Speech and Language Processing (Jurafsky & Martin)](https://web.stanford.edu/~jurafsky/slp3/)
- **과제/실습**
  1. NLTK/Python으로 텍스트 정규화 파이프라인 구현 (토큰화, 불용어 제거)
  2. 감성 분석(이진 분류) 프로젝트 with TF-IDF vs Word2Vec 임베딩

### Week 17-18: RNN 기반 NLP
- **학습 목표**
  - RNN, LSTM, GRU를 활용한 순차 데이터 처리
  - Seq2Seq, Attention (Bahdanau, Luong)
- **과제/실습**
  1. 간단한 기계번역(번역, 요약 등) Seq2Seq 모델 구현
  2. 어텐션 메커니즘 시각화 (Attention Weights)

### Week 19-20: 프로젝트 (전통적 NLP + 딥러닝 결합)
- **프로젝트 예시**
  1. **챗봇 프로젝트**: FAQ 데이터 기반, RNN+임베딩 조합
  2. **뉴스 기사 요약**: Seq2Seq로 Headlines 자동 생성
  3. **감성 분석 고도화**: Word2Vec + LSTM 기반
- **출력물**
  - Colab Notebook or local PyTorch/Keras 프로젝트
  - 데이터 전처리, 모델 구조, 성능 지표(F1, BLEU 등) 명시

---
## Part 4: LLM Scientist (21~28주차)

### Week 21-22: 트랜스포머 구조 & 사전 학습 모델
- **학습 목표**
  - Transformer 메커니즘(Self-Attention, Multi-Head Attention, Positional Encoding)
  - BERT, GPT 등 사전학습(Pretraining) & 파인튜닝(Fine-tuning)
- **학습 자료**
  - [Attention Is All You Need 논문](https://arxiv.org/abs/1706.03762)
  - [Hugging Face Transformers 문서](https://huggingface.co/docs/transformers)
- **과제/실습**
  1. mini-Transformer 구현(직접 or 참고) & 작은 데이터셋 실습
  2. Hugging Face로 BERT or GPT-2 파인튜닝 (문서 분류/생성)

### Week 23-24: 최신 LLM 기법(LoRA, PEFT, RAG, 등)
- **학습 목표**
  - LoRA, QLoRA, Adapters를 이용한 경량 파인튜닝
  - Retrieval-Augmented Generation(RAG)으로 검색 + 생성 결합
- **과제/실습**
  1. LoRA/PEFT 기법 사용해 GPT-2 파인튜닝, GPU 메모리 절감 효과 비교
  2. RAG 파이프라인 구축(문서 DB + 생성 모델)

### Week 25-26: 데이터 수집 & 분산 학습
- **학습 목표**
  - 대규모 텍스트 데이터 크롤링/클리닝
  - PyTorch Distributed, DeepSpeed, 모델 병렬화
- **과제/실습**
  1. 자체 데이터셋 생성(웹 스크래핑 or 공개 코퍼스)
  2. 분산 학습 환경 설정 & 샘플 모델 트레이닝

### Week 27-28: 오픈소스 LLM 탐구 & 개인 프로젝트
- **학습 목표**
  - LLaMA, Falcon, Mistral 등 다양한 오픈소스 LLM
  - 개인 혹은 팀 프로젝트 기획 & 구현
- **과제/실습**
  1. 원하는 오픈소스 LLM 로드 후, 성능 평가 및 비교
  2. 최종 프로젝트 기획: 대규모 데이터셋으로 파인튜닝 or RAG 기반 챗봇 등

---
## Part 5: LLM Engineer (29~36주차)

### Week 29-30: LangChain/LlamaIndex로 애플리케이션 개발
- **학습 목표**
  - LangChain, LlamaIndex를 이용한 빠른 프로토타이핑
  - Vector DB(FAISS, Pinecone 등) 연동
- **과제/실습**
  1. 개인 지식베이스 + LangChain으로 챗봇 구현
  2. Vector DB에 Embeddings 저장 후 검색 & Q&A

### Week 31-32: 모델 최적화 & 배포
- **학습 목표**
  - ONNX, TensorRT, Quantization 등으로 모델 경량화
  - 클라우드 배포(AWS, GCP, Hugging Face Spaces) 실습
- **과제/실습**
  1. 온프레미스 vs 클라우드 환경에서 속도 비교, 비용 분석
  2. Docker 컨테이너 생성, CI/CD 파이프라인 구축

### Week 33-34: 모델 해석(Interpretability) & MLOps
- **학습 목표**
  - Attention 시각화, Explainable AI(XAI) 기법
  - MLOps(모델 버전 관리, 로깅, 모니터링, 자동 재학습)
- **과제/실습**
  1. TensorBoard/Hugging Face Model Hub에서 모델 버전 추적
  2. 예측 결과가 왜 그렇게 나왔는지 시각화(Attention Map 등)

### Week 35-36: 최종 프로젝트 완성 & 실전 운영
- **학습 목표**
  - 실제 운영 환경(Prod) 배포, 사용자 피드백 반영, 문제 해결
  - 성능 지표 모니터링, 모델 업데이트 주기 설정
- **출력물**
  1. 완성된 웹/모바일 등 애플리케이션 (LLM 기반)
  2. 운영 경험 공유, 에러로그 분석, 추후 개선 계획

---
## 종합 학습 일정 (주차별 요약)

| 파트 | 주차 범위  | 주요 학습 내용                                                      |
|------|-----------|-------------------------------------------------------------------|
| **1. Fundamentals**    | 1~8주차   | Python, 수학/통계, ML 기초, 미니 프로젝트                                  |
| **2. Neural Networks**  | 9~14주차  | 퍼셉트론, MLP, CNN, RNN, 옵티마이저 & 규제화                                  |
| **3. NLP**              | 15~20주차 | 전통적 NLP, 텍스트 전처리, Word Embedding, Seq2Seq, Attention                |
| **4. LLM Scientist**    | 21~28주차 | Transformer, BERT/GPT, LoRA, RAG, 분산학습, 오픈소스 LLM                     |
| **5. LLM Engineer**     | 29~36주차 | LangChain/LlamaIndex, 최적화 & 배포, 모델 해석, MLOps, 최종 프로젝트           |

> 최소 8개월(36주) 분량으로 설계했으며, 일부 단계는 최대 12개월 이상으로 확장 가능

---
## 최종 안내

이 **실제 커리큘럼**은 주차별로 구체적인 공부 주제, 과제, 실습 프로젝트를 제시합니다. 학습 속도, 개인 사정, 리소스(하드웨어, GPU, 클라우드 비용 등)에 따라 조정하세요.

1. **데이터 윤리 & 저작권**: 대규모 텍스트 데이터 수집 시 법적 책임이 따를 수 있으니 주의.
2. **커뮤니티 & 협업**: 논문 독서, 스터디 그룹, 학회/세미나 참석, GitHub 기여 등을 병행.
3. **프로젝트 포트폴리오화**: 학습 결과물을 Github, 블로그, 학회 포스터 등으로 정리.
4. **지속적 업데이트**: NLP/LLM 분야는 빠르게 진화하므로, 관련 뉴스레터/논문을 꾸준히 팔로우.

위 커리큘럼을 기준으로 학습한다면, **기초 → 신경망 → NLP → LLM 연구 → 실제 서비스 배포**의 전 과정을 단계적으로 습득할 수 있을 것입니다. 모두에게 도움이 되길 바랍니다. **행운을 빕니다! 🚀**

