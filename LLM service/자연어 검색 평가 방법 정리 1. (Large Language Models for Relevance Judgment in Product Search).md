# Paper List
- **Large Language Models for Relevance Judgment in Product Search**
- Auto Arena of LLMs - Automating LLM Evaluations with Agent Peer-battles and Committee Discussions

# Paper Summary
## 1. Large Language Models for Relevance Judgment in Product Search
- 판단: 현재 진행하고 있는 프로젝트에 어울리지 않은 논문
	- 바로 사용 불가 (학습 데이터 사용 X, 학습 X)
-------------
### 00. keyword summary
1. QIP (Query-Item Pair)
	- 검색 시스템이 얼마나 적절하게 쿼리에 대응하는 아이템을 반환하는지를 평가하는 데 사용
	- 주요 목적
		- **관련성 평가**: 사용자가 입력한 쿼리와 그에 대한 검색 결과(아이템)의 관련성을 측정
		- **검색 시스템 성능 향상**: QIP는 검색 알고리즘의 성능을 개선하기 위한 학습 데이터로 사용
	- **예시 1**:
		- **Query(쿼리)**: “무선 블루투스 이어폰”
		- **Item(아이템)**: “Sony WF-1000XM4 무선 블루투스 이어폰”
		- **Label(라벨)**: **관련 있음**
	- **예시 2**:
		- **Query(쿼리)**: “애플 노트북”
		- **Item(아이템)**: “MacBook Pro 13인치 2022”
		- **Label(라벨)**: **관련**
	- **예시 3**:
		- **Query(쿼리)**: “고양이 사료”
		- **Item(아이템)**: “강아지 사료 5kg”
		- **Label(라벨)**: **관련 없음**
3. 
### 01. information retrieval (IR)
| **방법**                               | **정의**                                                                                                                               | **장점**                                                                                                            | **단점**                                                                                      |
| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| **2.1 Crowd Sourcing**               | 여러 비전문가 작업자들이 쿼리-아이템 쌍(QIP)의 관련성을 라벨링하는 방식. <br><br>온라인 도구를 통해 작업을 분배하고 관리함.                                                         | - 저렴한 비용으로 대규모 데이터 수집 가능<br><br> - 동시에 다수의 작업자가 작업 가능해 **빠른 처리** 가능                                               | - **작업자의 신뢰성**과 **품질 관리** 어려움<br><br> - 작업의 설정(예: 작업 길이, 보여지는 데이터 범위)에 따라 **편향된 라벨링** 발생 가능 |
| **2.2 Query-Product Classification** | **쿼리-아이템 쌍(QIP)**을 분류하는 **관련성 분류기**를 통해 자동화된 관련성 평가를 수행. <br><br>이를 위해 **고품질 라벨이 붙은 QIP 데이터셋**이 필요함.                                 | - 관련성 평가 **자동화**로 인한 **일관성 있는 평가** 가능<br><br> - 대규모 데이터에서 **효율적인 처리** 가능                                          | - **초기 시드 데이터셋**으로 고품질의 라벨링 데이터 필요<br><br> - 크라우드 소싱에서 발생하는 **불일치** 문제를 완전히 해결하지 못할 수 있음    |
| **2.3 LLM Use in Product Search**    | **대규모 언어 모델(LLMs)**을 활용해 **의미 기반 검색**과 **관련성 평가**를 수행. <br><br>LLM이 쿼리와 아이템 속성을 결합해 관련성을 예측하고, **자기 증류(self-distillation)** 기법을 사용함. | - LLM은 **장기 메모리 의존성** 및 **문맥적 관계**를 잘 분석해 관련성 판단이 가능<br><br> - **크로스 인코더 구조**로 높은 성능 달성 가능<br><br> - **비용 절감** 가능 | - LLM의 학습에는 **대규모의 학습 데이터**와 **시간**이 필요함<br><br> - LLM은 **고성능 하드웨어**와 **높은 비용**이 요구될 수 있음   |
### 02. 논문에서 사용하는 Data
#### 예시

| **Query**                  | **Relevant item**                                            | **Related item**                                                                           | **Irrelevant item**                                            |
| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | -------------------------------------------------------------- |
| graco 5 in 1 crib mattress | Graco Benton 5-in-1 Convertible Baby Crib with Drawer, White | Graco 5" Crib and Toddler Mattress with Ultra-Soft, Water-Resistant, Removable Outer Cover | Graco Pack ’n Play Playard Newborn Fitted Sheets, 2 Pack, Gust |
- **Query (쿼리)**: “graco 5 in 1 crib mattress” (그레이코 5-in-1 유아용 침대 매트리스)
- **Relevant item (관련 있음)**: “Graco Benton 5-in-1 Convertible Baby Crib with Drawer, White” (그레이코 벤튼 5-in-1 변환 가능한 유아용 침대, 서랍 포함, 화이트)
- **Related item (관련 있음(일부))**: “Graco 5” Crib and Toddler Mattress with Ultra-Soft, Water-Resistant, Removable Outer Cover” (그레이코 5인치 유아 및 아동용 매트리스, 초부드럽고, 방수 가능, 탈부착 가능한 외피)
- **Irrelevant item (관련 없음)**: “Graco Pack ’n Play Playard Newborn Fitted Sheets, 2 Pack, Gust” (그레이코 팩앤플레이 플레이아드 신생아용 맞춤형 시트, 2팩, 거스트)
#### Train Test split
 - 무작위 추출
 - Train: 6,055,251 pairs
 - Test: 66,072 pairs
#### Train 데이터 Detail

| **Metric**         | **Count** |
| ------------------ | --------- |
| total # of queries | 708,895   |
| total # of items   | 2,664,741 |

| **Label**  | **#**     | **%**  |
| ---------- | --------- | ------ |
| Relevant   | 3,887,072 | 64.19% |
| Related    | 1,191,913 | 19.68% |
| Irrelevant | 976,266   | 16.12% |
#### Test Data Labeling (golden label)
1. **기존 라벨 (First Annotation)**: 
   - 테스트 세트에 대해 이미 존재하는 첫 번째 라벨

2. **두 번째 라벨링 (Second Annotation)**:
   - 새로운 인간 주석자가 라벨을 다시 수집
   - 만약 첫 번째 라벨과 **일치**하면, **최종 라벨(Golden Label)로 넘어감
   - 만약 **불일치**하면, 세 번째 라벨링으로 진행

3. **세 번째 라벨링 (Third Annotation)**:
   - 첫 번째와 두 번째 라벨이 일치하지 않는 경우, 또 다른 인간 평가자가 다시 라벨링
   - 이 라벨이 **최종 라벨(Golden Label)로 결정됨
   
4. **최종 라벨(Golden Label)**:
   - 다수결에 의해 결정된 최종 라벨
   - 이를 기준으로 모델과 인간 평가자를 평가

- 기존 label의 신뢰 정도를 표현한 table

|                 | f1 class-0 | f1 class-1 | f1 class-2 | f1 micro |
| --------------- | ---------- | ---------- | ---------- | -------- |
| **Human perf.** | 0.890      | 0.862      | 0.968      | 0.937    |
#### Data Variations in Training

| #   | Data Variation Description                                      |
|-----|-----------------------------------------------------------------|
| #1  | title, product_type, brand, color, gender                       |
| #2  | #1 + item description, query per item with description ≈ 3      |
| #3  | #1 + item description, query per item with description ≈ 1      |
- **#1**: 가장 단순한 정보만으로 어느 정도의 성능을 낼 수 있는지 확인.
- **#2**: 정보가 더 풍부할 때, 쿼리가 여러 개 연결되었을 때 성능이 얼마나 향상되는지 분석.
- **#3**: 정보는 많지만, 쿼리 수가 적을 때 성능 저하가 있는지 확인.

예시 1: 기본 항목 속성만 포함된 경우 (#1)

| **Query**                  | **Item Title**                | **Attributes**                                      | **Relevance Label** |
|----------------------------|-------------------------------|-----------------------------------------------------|---------------------|
| "애플 노트북"               | "MacBook Air 13인치 2022"     | **제품 유형**: 노트북, **브랜드**: 애플             | 2 (관련 있음)        |
| "무선 블루투스 이어폰"      | "Sony WH-1000XM4"             | **제품 유형**: 이어폰, **브랜드**: 소니             | 2 (관련 있음)        |
예시 2: 쿼리-항목 연결이 많은 경우 (#2)

| **Query**     | **Item Title**          | **Attributes + Description**                                                   | **Relevance Label** |
| ------------- | ----------------------- | ------------------------------------------------------------------------------ | ------------------- |
| "애플 노트북"      | "MacBook Air 13인치 2022" | **제품 유형**: 노트북, **브랜드**: 애플, **설명**: "가볍고 슬림한 디자인, M1 칩 탑재, 13인치 Retina 디스플레이" | 2 (관련 있음)           |
| "최고 성능 노트북"   | "MacBook Air 13인치 2022" | **제품 유형**: 노트북, **브랜드**: 애플, **설명**: "가볍고 슬림한 디자인, M1 칩 탑재, 13인치 Retina 디스플레이" | 2 (관련 있음)           |
| "휴대하기 좋은 노트북" | "MacBook Air 13인치 2022" | **제품 유형**: 노트북, **브랜드**: 애플, **설명**: "가볍고 슬림한 디자인, M1 칩 탑재, 13인치 Retina 디스플레이" | 2 (관련 있음)           |
예시 3: 긴 텍스트 설명 포함된 경우 (#3)

| **Query**                  | **Item Title**                | **Attributes + Description**                                                            | **Relevance Label** |
|----------------------------|-------------------------------|------------------------------------------------------------------------------------------|---------------------|
| "애플 노트북"               | "MacBook Air 13인치 2022"     | **제품 유형**: 노트북, **브랜드**: 애플, **설명**: "가볍고 슬림한 디자인, M1 칩 탑재, 13인치 Retina 디스플레이" | 2 (관련 있음)        |
| "무선 블루투스 이어폰"      | "Sony WH-1000XM4"             | **제품 유형**: 이어폰, **브랜드**: 소니, **설명**: "최고의 소음 차단 기능, 장시간 착용 가능한 인체공학적 디자인" | 2 (관련 있음)        |
### 03. 논문에서 사용하는 모델
| **모델**   | **베이스 모델**          | **주요 특성 및 구현 방식**                                                                          | **훈련 방법**                                                                                   | **특징**                                                                                           |
| -------- | ------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| **LLM1** | **BERT 기반 크로스 인코더** | - **크로스 인코더 기반 모델**<br> - **Huggingface LLM1-Base** 사용<br> - **Walmart 검색 코퍼스**에서 추가 사전 학습 | - 인간이 라벨링한 **쿼리-아이템 관련성 쌍**을 사용한 미세 조정 <br> - **마스킹 언어 모델링** 및 **이진화된 주문 예측 작업** 수행         | - **일반 도메인** 텍스트로 훈련된 LLM1을 **전자상거래 텍스트**로 성능 향상 <br> - **15% 마스킹**과 **Walmart 행동 데이터** 사용       |
| **LLM2** | **BERT 기반**         | - **LLM1 개선 버전**                                                                           | - **LLM1과 동일한 방식**으로 사용                                                                     | - **LLM1**의 성능을 개선한 모델                                                                           |
| **LLM3** | **Llama 기반 모델**     | - **크로스 인코더 기반 모델** <br> - **LoRA** 사용 또는 비사용 버전 실험                                        | - **LoRA 순위**와 **알파(α)** 값 조정 <br> - 마지막 토큰 기반 분류 수행 <br> - **분류기(Dense Layer)**를 추가해 미세 조정 | - **Low Rank Adaptation(LoRA)**을 통해 성능 향상 <br> - 모델 파라미터 수와 성능 최적화를 위한 실험 진행                     |
| **LLM4** | **Mistral 기반 모델**   | - **LLM1, LLM3와 유사한 방식** <br> - **7B 파라미터 모델**                                             | - 다양한 **학습률 스케줄러** 실험 <br> - **제품 설명 포함 여부** 및 **제품 특징 드롭아웃** <br> - Walmart 데이터로 도메인 적응 훈련 | - **Walmart 데이터**에서 **인과적 언어 모델(Causal LM)**로 사전 학습 후 **LoRA 미세 조정** <br> - **랜덤 네거티브** 도입 실험 포함 |
- 모델 학습 프레임워크: PyTorch, PyTorch Lightning, Huggingface Transformers
- 중간 크기의 모델 선택
	- **LoRA의 순위가 증가**함에 따라 성능이 감소하기 시작
	- LoRA이 전체 모델 미세 조정보다 더 나은 성능
	- peft 라이브러리 사용
- 학습 평가 비교 리스트
	1. **LoRA**(peft 라이브러리 사용)와 전체 모델 미세 조정 간의 비교
	2. **프롬프트 기반** 분류와 **마지막 토큰 기반** 분류
	3. 훈련 중 **아이템 설명 수준**
	4. **하이퍼 파라미터 선택**:
		a) (LoRA) **드롭아웃**
		b) **LoRA 순위** 및 **알파**와 훈련된 레이어
		c) **학습률(lr)** 및 **학습률 스케줄러**
		d) **optimizer**의 변형
	5. **모델 선택 및 양자화**
	6. **라벨 클래스 가중치**
### 04. 모델 학습 및 결과 비교
#### 정리
1. **자세한 항목 속성 데이터**를 포함하는 것이 쿼리 수에 따라 훈련 성능에 도움이 됩니다.
2. **전체 미세 조정**이 **LoRA 근사**보다 성능이 뛰어나지 않습니다.
3. **LoRA 훈련에서 α 값**이 **순위의 절반**일 때 가장 좋은 성능을 보였습니다.
4. **LoRA 드롭아웃**과 **학습률 스케줄러**는 훈련 성능에 효과적으로 영향을 미쳤습니다.
5. **더 큰 언어 모델**은 **아이템 텍스트 정보**가 증가할수록 성능이 더 개선됩니다.

> **LoRA가 전체 미세 조정보다 더 높은 성능**을 보인 이유는 **약 600만 개의 QIP**를 사용하는 중간 크기 훈련 데이터셋과 **7B 파라미터** 모델 크기 사이의 균형 덕분입니다.

