### keyword
- Prior
- Conditional Probabilities
- Posterior Probabilities
### Bayes Decision Theory 란?
- 기존에 가지고 있던 사전 정보를 활용하여 의사결정할때 사용되는 이론
### Concept 1. Prior
- 사전 정보 없이 알 수 있는 확률 정보
- 새로운 정보가 제공되기 전에 이벤트가 발생할 확률
### Concept 2. Conditional Probabilities
- 주어진 상태 x 에서 변하는 y 값의 확률 분포
$$
p(y|x)
$$
![[Figure_1.png]]
### Concept 3. Posterior Probabilities
- 어떠한 사전 정보가 주어졌을 떄 어떤 결정을 해야하는지를 알려주는 확률
- 베이즈 정리
$$
p(y|x) = \frac{p(x|y)\times p(y)}{p(x)} = \frac{p(x|y)\times p(y)}{\sum_i{p(x|y_i)p(y_i)}}
$$
$$
p(x) = \sum_i{p(x|y_i)p(y_i)}
$$
$$
posterior = \frac{Likelihood \times Prior}{Normalization \ Factor}
$$
### Decision Making
- 결정 하는 기준에 따라서 여러 가지 방법론이 나뉜다.
#### 1. Minimize the probability of misclassification
- misclassification 확률을 낮추는 것을 목적
![](https://blog.kakaocdn.net/dn/biyqhQ/btqy1gB4crU/1BYfAQ8rOTD9xDWID3p4Fk/img.png)
![](https://blog.kakaocdn.net/dn/bvo90P/btqyZtiAk55/0rYhL8hKvyXa9eKq59NBCK/img.png)
#### 2. Minimizing the Expected Loss
- expected loss 를 최소화하는 방향으로 의사결정을 한다.
 ![](https://blog.kakaocdn.net/dn/72EsM/btqy1gPB8SH/pZxtjovVcKFMyygCtFA9b1/img.png)
- posterior probability에 loss function을 곱한 값이 최소가 되는 방향으로 의사결정
#### 3. Reject Option
- 확실한 영역을 제외한 중간의 애매한 부분은 reject region 이라고 정의
- 이 영역에 있을시 결정은 사람이 직접 판단
![[Figure_2.png]]

### 과제

- 두 개의 클래스 $C_1, C_2$가 있으며, 각 클래스의 Prior(사전 확률)는 다음과 같습니다.
$$P(C_1)=0.4, P(C_2)=0.6$$
- 조건부 확률(Likelihood)은 아래와 같습니다.
$$
p(x|C_1)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{(x-\mu_1)^2}{2\sigma_1^2}\right),\quad \mu_1=2,\;\sigma_1=1
$$
$$
p(x|C_2)=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{(x-\mu_2)^2}{2\sigma_2^2}\right),\quad \mu_2=4,\;\sigma_2=1.5
$$

- 다음 질문에 손으로 직접 계산하여 답하시오.
	1. 입력 값 $x=3$일 때 각 클래스의 Likelihood를 계산하시오.
	2. $x=3$일 때의 Posterior 확률을 베이즈 정리를 사용해 계산하시오.
	3. 위의 계산을 기반으로 $x=3$의 분류 결과와 그 근거를 서술하시오.
	4. 입력 값 $x=5$일 때 위 과정을 반복하여 다시 분류하시오.

---

### 과제 풀이

#### 1. Likelihood 계산 $(x=3)$
- 클래스 $C_1$: $p(3|C_1)\approx0.24197$
- 클래스 $C_2$: $p(3|C_2)\approx0.21297$

#### 2. Posterior 확률 계산 ($x=3$)
- $P(x=3)=0.24197\times0.4+0.21297\times0.6=0.22457$
- 클래스 $C_1$: $P(C_1|3)=\frac{0.24197\times0.4}{0.22457}\approx0.4310$
- 클래스 $C_2$: $P(C_2|3)=\frac{0.21297\times0.6}{0.22457}\approx0.5690$

#### 3. 분류 결과 ($x=3$)
- Posterior 확률이 더 높은 클래스 $C_2$로 분류
- $P(C_1|3)=0.4310$
- $P(C_2|3)=0.5690$ ✅

- >> **결과: 클래스 $C_2$**
#### 4. 반복 계산 ($x=5$)

- 클래스 $C_1$: $p(5|C_1)\approx0.00443$
- 클래스 $C_2$: $p(5|C_2)\approx0.21297$

- $P(x=5)=0.00443\times0.4+0.21297\times0.6=0.12955$

- 클래스 $C_1$: $P(C_1|5)=\frac{0.00443\times0.4}{0.12955}\approx0.0137$
- 클래스 $C_2$: $P(C_2|5)=\frac{0.21297\times0.6}{0.12955}\approx0.9863$

- Posterior 확률이 더 높은 클래스 $C_2$로 분류

- $P(C_1|5)=0.0137$
- $P(C_2|5)=0.9863$ ✅

- >> **결과: 클래스 $C_2$**

---
#### 최종 정리

| 입력값(x) | $P(C_1\|x)$ | $P(C_2\|x)$ | 분류 결과       |
| ------ | ----------- | ----------- | ----------- |
| 3      | 0.4310      | 0.5690 ✅    | **Class 2** |
| 5      | 0.0137      | 0.9863 ✅    | **Class 2** |


### reference
- https://process-mining.tistory.com/81